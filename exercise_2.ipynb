{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "de1fea42-c667-4363-83ee-975c771fad50",
      "metadata": {
        "id": "de1fea42-c667-4363-83ee-975c771fad50"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vischia/lisbon-ml-school/blob/master/exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8afc560-312c-40e5-8188-62dc4c952efa",
      "metadata": {
        "id": "c8afc560-312c-40e5-8188-62dc4c952efa"
      },
      "source": [
        "# Lisbon Machine Learning School\n",
        "## Exercise 2: Classification, showcase with CMS S-top Search: Signal events vs Background events\n",
        "\n",
        "(C) Cristóvão B. da Cruz e Silva (Laboratório de Instrumentação e Física Experimental de Partículas), cbeiraod@cern.ch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zHFYzPwluV16",
      "metadata": {
        "id": "zHFYzPwluV16"
      },
      "source": [
        "Dislcaimer: some content and examples shamelessly \"stolen\" from Pietro Vischia's Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04fe62fe-03ac-4127-a09a-a1340ae0f62a",
      "metadata": {
        "id": "04fe62fe-03ac-4127-a09a-a1340ae0f62a"
      },
      "source": [
        "## Setup the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d24545-63c7-4594-ac92-0eb3de9d1929",
      "metadata": {
        "id": "22d24545-63c7-4594-ac92-0eb3de9d1929"
      },
      "source": [
        "- If you are running locally, you don't need to run anything, but make sure to point the base_data_dir variable to the directory where you have the data\n",
        "\n",
        "- If you are running on Google Colab, uncomment and run the next cell (remove only the \"#\", keep the \"!\"). You can also run it from a local installation, but it will do nothing if you have already installed all dependencies (and it will take some time to tell you it is not gonna do anything).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c349fc5-c41c-4910-a16a-adc6068d7a1f",
      "metadata": {
        "id": "8c349fc5-c41c-4910-a16a-adc6068d7a1f"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#%cd \"/content/drive/MyDrive/\"\n",
        "#! git clone https://github.com/vischia/lisbon-ml-school.git\n",
        "#%cd lisbon-ml-school\n",
        "#!pwd\n",
        "#!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YsZ1ehzJm2ly",
      "metadata": {
        "id": "YsZ1ehzJm2ly"
      },
      "source": [
        "This exercise follows from the CMS analysis [https://doi.org/10.1007/JHEP09(2018)065][https://arxiv.org/abs/1805.05784] in search of pair production of the supersymmetric partner of the top quark in the situation where the difference in mass between the s-top quark and the lightest supersymmetric particle (LSP) is smaller than the W mass, in this situation the s-top quark undergoes a \"four-body decay\".\n",
        "\n",
        "![StopProduction](figs/StopProd.png)\n",
        "\n",
        "The paper above focuses on the data from 2016, while the analysis was later extended to the full Run II dataset, [https://doi.org/10.1007/JHEP06(2023)060][https://arxiv.org/abs/2301.08096].\n",
        "\n",
        "Both papers use a BDT machine learning algorithm for classifying background and signal events. One BDT is trained for each $\\Delta M$ region, where $\\Delta M$ is defined as the difference in mass between the s-top and the LSP, typically a neutralino. As a result, 8 different BDTs are trained for these analysis, for each year of data taking (the years are treated independently).\n",
        "\n",
        "SUSY models are characterised by large phase spaces which must be excluded. For these analysis a simplified model spectra model for the signal samples were used, in these models the SM is extended only by the minimal particles and interactions thought to be of relevance for what is under study, in this case the s-top and the LSP as well as their corresponding vertices in the diagram above. This still provides quite a large phase space for the analysis search, in particular the phase space covers possible masses for the LSP vs possible masses for the s-top. As a result, merging of the signal samples is desirable to reduce the analysis complexity. It can also be observed that to first order, the kinematics of the decay products depend on the $\\Delta M$ quantity, so splitting by $\\Delta M$ is a reasonable approach.\n",
        "\n",
        "Given the small mass difference between the s-top and the LSP, the decay prodcuts of this channel are typically quite soft and may not fully pass strict selection criteria. For this reason the analysis requires the event to have one hard jet, this jet is interpreted to be a jet originating from Initial State Radiation (ISR) and provides a boost to the decay products of the s-top decay. For this same reason the event selection only requires one lepton and one jet from the event together with missing transverse energy, the presence of b-jets, while necessary according to the diagram is not enforced due to the low efficiency. However, the classifier of the jet with the highest b-tag is passed to the machine learning algorithm in order to provide information about the b-jet content to the ML approach. The preselection is mostly uniform for all $\\Delta M$ regions, with only $\\Delta M = {70,80}$ requiring a slightly tuned preselection to account for the more challenging kinematics in this region.\n",
        "\n",
        "An iterative procedure was used in the development of these analysis and a subset of 12 input features to the BDT were observed to deliver the best performance. The variable are:\n",
        " - Jet1Pt - Transverse momentum of the leading jet\n",
        " - mt - Transverse mass\n",
        " - Met - Missing transverse energy\n",
        " - LepChg - Charge of the leading lepton\n",
        " - LepEta - Eta of the leading lepton\n",
        " - LepPt - Transverse momentum of the leading lepton\n",
        " - HT - sum of the transverse momentum of all particles in the event\n",
        " - NbLoose - number of jets passing loose b-jet criteria\n",
        " - Njet - number of jets\n",
        " - JetHBpt - Transverse momentum of the jet with the highest b-tag score\n",
        " - DrJetHBLep - $\\Delta R$ between the jet with the highest b-tag score and the leading lepton\n",
        " - JetHBCSV - B-tag score of the jet with the highest b-tag score\n",
        "\n",
        "Included in the nTuples are several other variables considered to be possibly useful for the analysis, an invested individual could explore the use of these other features in addition or in alternative to the ones aboves.\n",
        "\n",
        "In this exercise we will reimplement this analysis using a Neural Network algorithm and compare the performance to that of the BDT approach used in the papers. The exact same BDT classifiers used in the 2016 analysis were applied to these MC samples and the output is stored in a separate nTuple and loaded as an additional column, providing us easy access to perform a preliminary comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LVPKEFoQuggZ",
      "metadata": {
        "id": "LVPKEFoQuggZ"
      },
      "source": [
        "## Fetch the data\n",
        "\n",
        "If you have not yet retrieved the data and placed it into an appropriate folder you can access either from google drive or on your local computer, follow the equivalent steps below.\n",
        "\n",
        "The steps below should work for colab without any modification, you will need to modify them to run locally on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iWn2gkVbuf0x",
      "metadata": {
        "id": "iWn2gkVbuf0x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# To run locally:\n",
        "if not os.path.isdir(\"/content/drive/MyDrive/\") and not os.path.isdir(\"data/Lisbon_ML_School_Stop\"):\n",
        "    !cd data/; wget https://cernbox.cern.ch/remote.php/dav/public-files/fBw0wnuLt1CWFGa/Lisbon_ML_School_Stop.tar.gz; tar xzvf Lisbon_ML_School_Stop.tar.gz; cd -;\n",
        "# To run in colab:\n",
        "if os.path.isdir(\"/content/drive/MyDrive/\") and not os.path.isfile(\"/content/drive/MyDrive/Lisbon_ML_School_Stop/Data/ZZ.root\"):\n",
        "    !cd /content/drive/MyDrive; wget https://cernbox.cern.ch/remote.php/dav/public-files/fBw0wnuLt1CWFGa/Lisbon_ML_School_Stop.tar.gz; tar xzvf Lisbon_ML_School_Stop.tar.gz; cd -;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40176774-7f04-4172-a635-0db7bf55e304",
      "metadata": {
        "id": "40176774-7f04-4172-a635-0db7bf55e304"
      },
      "source": [
        "## Load the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6seizPFuj-VZ",
      "metadata": {
        "id": "6seizPFuj-VZ"
      },
      "outputs": [],
      "source": [
        "if os.path.isdir(\"/content/drive/MyDrive/\"): # Only on colab, locally you should have already installed these packages via conda\n",
        "    !pip install torchinfo\n",
        "    !pip install uproot\n",
        "    !pip install pyhf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f137b35-7bf7-446f-8c49-8487222a64fd",
      "metadata": {
        "id": "3f137b35-7bf7-446f-8c49-8487222a64fd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchinfo\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "\n",
        "import uproot\n",
        "\n",
        "import pandas\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
        "matplotlib.rcParams['axes.labelsize'] = 14\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    torch.set_default_dtype(torch.float32)\n",
        "\n",
        "print('Using torch version', torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d75588-f79b-488f-9198-93ec2f70b967",
      "metadata": {
        "id": "74d75588-f79b-488f-9198-93ec2f70b967"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb19ea71-cb59-48c1-a235-491307ba7156",
      "metadata": {
        "id": "fb19ea71-cb59-48c1-a235-491307ba7156"
      },
      "source": [
        "We use the [uproot](https://uproot.readthedocs.io/en/latest/basic.html) library to conveniently read in a [ROOT TNuple](https://root.cern.ch/doc/master/classTNtuple.html) which can automatically convert it to a [pandas dataframe](https://pandas.pydata.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y9blU41EofQ_",
      "metadata": {
        "id": "Y9blU41EofQ_"
      },
      "outputs": [],
      "source": [
        "def load_dataframe(base_dir, dataset, bdt = True, treename = 'data', previous_dataframes = None, do_split = True, train_factor=0.5, seed = None, category=None, subcategory=None):\n",
        "  if seed is not None:\n",
        "    np.random.seed(seed = seed)\n",
        "  if previous_dataframes is not None and dataset in previous_dataframes:\n",
        "    df = previous_dataframes[dataset]\n",
        "  elif dataset[-5:] == '.root':\n",
        "    # Load base data\n",
        "    df = uproot.open(os.path.join(base_dir, 'Data', dataset))[treename].arrays(library='pd')\n",
        "    #df = uproot.open(os.path.join(base_dir, 'DataC', dataset))[treename].arrays(library='pd') # DataC may be better optimized for ML, maybe the students can check\n",
        "\n",
        "    # Load friend trees\n",
        "    if bdt:\n",
        "      df_bdt = uproot.open(os.path.join(base_dir, 'BDT', dataset))[treename].arrays(library='pd')\n",
        "      df = pandas.concat([df,df_bdt],axis=1)\n",
        "      del df_bdt\n",
        "\n",
        "    # Apply final selections\n",
        "    # Preselection is already applied:\n",
        "    #     \"(tightLepton) && (DPhiJet1Jet2 < 2.8 || Jet2Pt < 67) && (HT > 181) && (Met > 254) && (Jet1Pt > 100)\",\n",
        "    # For High DM (only 70 and 80), apply final selection of:\n",
        "    #     \"(DPhiJet1Jet2 < 2.5 || Jet2Pt < 60) && (HT > 200) && (Met > 280) && (Jet1Pt > 110)\",\n",
        "    # For Low DM, apply final selection of:\n",
        "    #     \"(DPhiJet1Jet2 < 2.5 || Jet2Pt < 60) && (HT > 200) && (Met > 280) && (Jet1Pt > 110) && (LepPt < 30)\",\n",
        "    #sel = (df['DPhiJet1Jet2'] < 2.5) | (df['Jet2Pt'] < 60) & (df['HT'] > 200) & (df['Met'] > 280) & (df['Jet1Pt'] > 110)\n",
        "    sel = (df['DPhiJet1Jet2'] < 2.5) | (df['Jet2Pt'] < 60) & (df['HT'] > 200) & (df['Met'] > 280) & (df['Jet1Pt'] > 110) & (df['LepPt'] < 30)\n",
        "    df = df.loc[sel]\n",
        "\n",
        "    # Split train/test\n",
        "    if do_split:\n",
        "      2+2\n",
        "      #df['isTrain'] = np.random.choice([True, False], size=len(df), p=[train_factor, 1-train_factor])\n",
        "\n",
        "    # Apply ne columns\n",
        "    if category is not None:\n",
        "      df['category'] = category\n",
        "    if subcategory is not None:\n",
        "      df['subcategory'] = subcategory\n",
        "  else:\n",
        "    if len(dataset) > 2 and dataset[:2] == 'DM' and int(dataset[2:]) in [10, 20, 30, 40, 50, 60, 70, 80]:\n",
        "      dm = int(dataset[2:])\n",
        "      ds_list = []\n",
        "      for stopM in range(250, 801, 25):\n",
        "        neutM = stopM - dm\n",
        "        ds_list.append((f'T2DegStop_{stopM}_{neutM}.root', f'Signal_{stopM}_{neutM}'))\n",
        "    elif dataset == 'DYJets':\n",
        "      ds_list = [\n",
        "          'DYJetsToLL_M5to50_HT100to200.root',\n",
        "          'DYJetsToLL_M5to50_HT200to400.root',\n",
        "          'DYJetsToLL_M5to50_HT400to600.root',\n",
        "          'DYJetsToLL_M5to50_HT600toInf.root',\n",
        "          'DYJetsToLL_M50_HT100to200.root',\n",
        "          'DYJetsToLL_M50_HT200to400.root',\n",
        "          'DYJetsToLL_M50_HT400to600.root',\n",
        "          'DYJetsToLL_M50_HT600to800.root',\n",
        "          'DYJetsToLL_M50_HT800to1200.root',\n",
        "          'DYJetsToLL_M50_HT1200to2500.root',\n",
        "          'DYJetsToLL_M50_HT2500toInf.root',\n",
        "      ]\n",
        "    elif dataset == 'QCD':\n",
        "      ds_list = [\n",
        "          'QCD_HT50to100.root',\n",
        "          'QCD_HT100to200.root',\n",
        "          'QCD_HT200to300.root',\n",
        "          'QCD_HT300to500.root',\n",
        "          'QCD_HT500to700.root',\n",
        "          'QCD_HT700to1000.root',\n",
        "          'QCD_HT1000to1500.root',\n",
        "          'QCD_HT1500to2000.root',\n",
        "          'QCD_HT2000toInf.root',\n",
        "      ]\n",
        "    elif dataset == 'SingleT':\n",
        "      ds_list = [\n",
        "          'T_tch_powheg.root',\n",
        "          'T_tWch_ext.root',\n",
        "          'TBar_tch_powheg.root',\n",
        "          'TBar_tWch_ext.root',\n",
        "      ]\n",
        "    elif dataset == 'TT_Pow':\n",
        "      ds_list = [\n",
        "          #'TT_pow_backup.root',\n",
        "          'TT_pow.root',\n",
        "      ]\n",
        "    elif dataset == 'TTLep':\n",
        "      ds_list = [\n",
        "          'TTJets_DiLepton.root',\n",
        "          'TTJets_SingleLeptonFromT.root',\n",
        "          'TTJets_SingleLeptonFromTbar.root',\n",
        "      ]\n",
        "    elif dataset == 'TTX':\n",
        "      ds_list = [\n",
        "          'TTGJets.root',\n",
        "          'TTWToLNu.root',\n",
        "          'TTWToQQ.root',\n",
        "          'TTZToLLNuNu_m1to10.root',\n",
        "          'TTZToLLNuNu.root',\n",
        "          'TTZToQQ.root',\n",
        "      ]\n",
        "    elif dataset == 'WJets':\n",
        "      ds_list = [\n",
        "          'Wjets_70to100.root',\n",
        "          'Wjets_100to200.root',\n",
        "          'Wjets_200to400.root',\n",
        "          'Wjets_400to600.root',\n",
        "          'Wjets_600to800.root',\n",
        "          'Wjets_800to1200.root',\n",
        "          'Wjets_1200to2500.root',\n",
        "          'Wjets_2500toInf.root',\n",
        "      ]\n",
        "    elif dataset == 'Diboson':\n",
        "      ds_list = [\n",
        "          'WW.root',\n",
        "          'WZ.root',\n",
        "          'ZZ.root',\n",
        "      ]\n",
        "    elif dataset == 'ZInv':\n",
        "      ds_list = [\n",
        "          'ZJetsToNuNu_HT100to200.root',\n",
        "          'ZJetsToNuNu_HT200to400.root',\n",
        "          'ZJetsToNuNu_HT400to600.root',\n",
        "          'ZJetsToNuNu_HT600to800.root',\n",
        "          'ZJetsToNuNu_HT800to1200.root',\n",
        "          'ZJetsToNuNu_HT1200to2500.root',\n",
        "          'ZJetsToNuNu_HT2500toInf.root',\n",
        "      ]\n",
        "      #'TTW_LO.root',\n",
        "      #'TTZ_LO.root',\n",
        "    elif dataset == \"Background_TTPow\":\n",
        "      ds_list = [\n",
        "          'TT_Pow',\n",
        "          'WJets',\n",
        "      ]\n",
        "    elif dataset == \"Background_TTPow_ZInv\":\n",
        "      ds_list = [\n",
        "          'TT_Pow',\n",
        "          'WJets',\n",
        "          'ZInv',\n",
        "      ]\n",
        "    elif dataset == \"Background_TTLep\":\n",
        "      ds_list = [\n",
        "          'TTLep',\n",
        "          'WJets',\n",
        "      ]\n",
        "    elif dataset == \"Background_TTLep_ZInv\":\n",
        "      ds_list = [\n",
        "          'TTLep',\n",
        "          'WJets',\n",
        "          'ZInv',\n",
        "      ]\n",
        "    else:\n",
        "      raise RuntimeError(f\"Unable to find dataset {dataset}\")\n",
        "    df = None\n",
        "    for ds in ds_list:\n",
        "      if isinstance(ds, tuple):\n",
        "        subcat = ds[1]\n",
        "        ds = ds[0]\n",
        "      else:\n",
        "        subcat = subcategory\n",
        "      new_df = load_dataframe(base_dir, ds, bdt = bdt, treename = treename, previous_dataframes = previous_dataframes, category=category, subcategory=subcat)\n",
        "      if previous_dataframes is not None:\n",
        "        if ds not in previous_dataframes:\n",
        "          previous_dataframes[ds] = new_df\n",
        "      if df is None:\n",
        "        df = new_df\n",
        "      else:\n",
        "        df = pandas.concat([df, new_df], ignore_index=True)\n",
        "        if previous_dataframes is None:\n",
        "          del new_df\n",
        "  #df.set_index('Event', inplace=True)\n",
        "  return df\n",
        "\n",
        "def load_all_dataframes(base_dir, bdt = True, treename = 'data', seed = None):\n",
        "  if seed is not None:\n",
        "    np.random.seed(seed = seed)\n",
        "\n",
        "  dataframes = {}\n",
        "\n",
        "  # Signal samples per \\DeltaM\n",
        "  #dataframes['DM10'] = load_dataframe(base_dir, \"DM10\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "  #dataframes['DM20'] = load_dataframe(base_dir, \"DM20\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "  dataframes['DM30'] = load_dataframe(base_dir, \"DM30\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Signal\")\n",
        "  #dataframes['DM40'] = load_dataframe(base_dir, \"DM40\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "  #dataframes['DM50'] = load_dataframe(base_dir, \"DM50\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "  #dataframes['DM60'] = load_dataframe(base_dir, \"DM60\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "  #dataframes['DM70'] = load_dataframe(base_dir, \"DM70\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "  #dataframes['DM80'] = load_dataframe(base_dir, \"DM80\", bdt = bdt, treename = treename, previous_dataframes = dataframes)\n",
        "\n",
        "  # Example points from different DM\n",
        "  # DM 10\n",
        "  dataframes['T2DegStop_250_240.root'] = load_dataframe(base_dir, \"T2DegStop_250_240.root\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Signal\", subcategory=\"Signal_250_240\")\n",
        "  # DM 30\n",
        "  dataframes['T2DegStop_250_220.root'] = load_dataframe(base_dir, \"T2DegStop_250_220.root\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Signal\", subcategory=\"Signal_250_220\")\n",
        "  # DM 50\n",
        "  dataframes['T2DegStop_250_200.root'] = load_dataframe(base_dir, \"T2DegStop_250_200.root\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Signal\", subcategory=\"Signal_250_200\")\n",
        "  # DM 80\n",
        "  dataframes['T2DegStop_250_170.root'] = load_dataframe(base_dir, \"T2DegStop_250_170.root\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Signal\", subcategory=\"Signal_250_170\")\n",
        "\n",
        "  # Individual MC samples\n",
        "  dataframes['DYJets'] = load_dataframe(base_dir, \"DYJets\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"DYJets\")\n",
        "  dataframes['QCD'] = load_dataframe(base_dir, \"QCD\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"QCD\")\n",
        "  dataframes['SingleT'] = load_dataframe(base_dir, \"SingleT\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"SingleTop\")\n",
        "  dataframes['TT_Pow'] = load_dataframe(base_dir, \"TT_Pow\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"TT\")\n",
        "  dataframes['TTLep'] = load_dataframe(base_dir, \"TTLep\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"TT\")\n",
        "  dataframes['TTX'] = load_dataframe(base_dir, \"TTX\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"TTX\")\n",
        "  dataframes['WJets'] = load_dataframe(base_dir, \"WJets\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"WJets\")\n",
        "  dataframes['Diboson'] = load_dataframe(base_dir, \"Diboson\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"VV\")\n",
        "  dataframes['ZInv'] = load_dataframe(base_dir, \"ZInv\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"ZInv\")\n",
        "\n",
        "  # Choose your favorite background model, but only 1\n",
        "  #dataframes['Background'] = load_dataframe(base_dir, \"Background_TTPow\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"Other\")\n",
        "  #dataframes['Background'] = load_dataframe(base_dir, \"Background_TTPow_ZInv\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"Other\")\n",
        "  dataframes['Background'] = load_dataframe(base_dir, \"Background_TTLep\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"Other\")\n",
        "  #dataframes['Background'] = load_dataframe(base_dir, \"Background_TTLep_ZInv\", bdt = bdt, treename = treename, previous_dataframes = dataframes, category=\"Background\", subcategory=\"Other\")\n",
        "\n",
        "  return dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ly89ByTvkR83",
      "metadata": {
        "id": "Ly89ByTvkR83"
      },
      "outputs": [],
      "source": [
        "base_data_dir=None\n",
        "\n",
        "# To run locally:\n",
        "if not os.path.isdir(\"/content/drive/MyDrive/\") and os.path.isdir(\"data/Lisbon_ML_School_Stop\"):\n",
        "    base_data_dir = 'data/Lisbon_ML_School_Stop'\n",
        "if os.path.isdir(\"/content/drive/MyDrive/\") and os.path.isfile(\"/content/drive/MyDrive/Lisbon_ML_School_Stop/Data/ZZ.root\"):\n",
        "    base_data_dir = '/content/drive/MyDrive/Lisbon_ML_School_Stop'\n",
        "\n",
        "\n",
        "dataframes = load_all_dataframes(base_data_dir, seed = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gTEojCSn8qy6",
      "metadata": {
        "id": "gTEojCSn8qy6"
      },
      "outputs": [],
      "source": [
        "list(dataframes.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d631f2-679f-41e5-a4dc-b8ba2c657add",
      "metadata": {
        "id": "f3d631f2-679f-41e5-a4dc-b8ba2c657add"
      },
      "source": [
        "## Inspect the data\n",
        "\n",
        "Before applying any \"fancy\" methods to the data, it is better to understand what the data is and looks like. **Spend some time exploring the data**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083a320d-365f-4f08-ac6a-01ca836ae57a",
      "metadata": {
        "id": "083a320d-365f-4f08-ac6a-01ca836ae57a"
      },
      "source": [
        "We have loaded several dataframes with simulated data for different processes, of particular interest to us are the signal dataframes and background dataframe. All dataframes have the same structure, let's start by looking at the structure of one of the dataframes:\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c0a89a-f7ef-4fa0-9014-d3ee0d0460b1",
      "metadata": {
        "id": "c4c0a89a-f7ef-4fa0-9014-d3ee0d0460b1"
      },
      "outputs": [],
      "source": [
        "print(dataframes['T2DegStop_250_220.root'].columns)\n",
        "print(len(dataframes['T2DegStop_250_220.root']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1849782e-6db2-43cd-a597-03377b23e08d",
      "metadata": {
        "id": "1849782e-6db2-43cd-a597-03377b23e08d"
      },
      "source": [
        "Most of the variables are reconstructed properties of the event, typically properties of the reconstructed particles. There are a number of special variables:\n",
        "\n",
        "- `BDT*`: these 8 variables are the result of applying the BDT methods from the publication to the event data, remember there are 8 different BDTs, one for each $\\Delta M$, when analysing signal samples, we should only look at the corresponding BDT variable and when analysing background samples we should look at the BDT variable corresponding to the $\\Delta M$ region we are analysing. For this exercise, we are considering only one region, $\\Delta M = 30\\,\\text{GeV}$.\n",
        "- `Run`, `Event`, `LumiSec`: these 3 variables uniquely identify each event, and are not really important for this exercise\n",
        "- `Nevt`, `genWeight`, `sumGenWeight`: are variables necessary for correctly weighing simulated events\n",
        "- `XS`: is the cross section to consider for the process when calculating yields\n",
        "- `weight`: is the weight to apply to the event for calculating yields"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50357d82-bbf6-4e56-b297-3b222075f306",
      "metadata": {
        "id": "50357d82-bbf6-4e56-b297-3b222075f306"
      },
      "source": [
        "### Plotting histograms of some observables using plotly\n",
        "Plotly is built on top of matplotlib and provides a bunch of useful and visually pleasing plotting options, it also offers interactive graphs which are great when exploring your data. See examples on [plotly](https://plotly.com/python/) website\n",
        "\n",
        "Plotly is excellent for providing interactive figures but this comes at the cost of the raw information having to be stored together with the figure. Plotly figures can use a lot of space and be very slow if there are many points. For this reason we limit the number of events in each plot below, by sampling from the dataframes, which should still give us a statistically relevant representation.\n",
        "\n",
        "Below we take a look at the $p_T$ distribution of the leading lepton for one of the signal samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "twDGd6Pfch8K",
      "metadata": {
        "id": "twDGd6Pfch8K"
      },
      "outputs": [],
      "source": [
        "df = dataframes['T2DegStop_250_220.root'].sample(n=6000)\n",
        "fig = px.histogram(\n",
        "    df,\n",
        "    x='LepPt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Leading Lepton Transverse Momentum',\n",
        "    labels={'LepPt': 'Lepton 1 $p_T$ [GeV]'},\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=100.0,\n",
        "        size=1\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "fig.show()\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6R6RxdLvi_Ah",
      "metadata": {
        "id": "6R6RxdLvi_Ah"
      },
      "source": [
        "We can compare the signal sample against one or more background samples, here we consider the leading Jet $p_T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JQ_hyqKOjbqD",
      "metadata": {
        "id": "JQ_hyqKOjbqD"
      },
      "outputs": [],
      "source": [
        "df = pandas.concat([dataframes['T2DegStop_250_220.root'].sample(n=6000), dataframes['WJets'].sample(n=12000)])\n",
        "fig = px.histogram(\n",
        "    df,\n",
        "    x='Jet1Pt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Leading Jet Transverse Momentum - WJets vs Signal',\n",
        "    labels={'Jet1Pt': 'Jet 1 $p_T$ [GeV]'},\n",
        "    color=\"subcategory\",\n",
        "    barmode=\"overlay\", # Without this option, plotly will stack the histograms by default\n",
        "    #marginal=\"rug\",\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=800.0,\n",
        "        size=4\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_PnJqH9qnSLI",
      "metadata": {
        "id": "_PnJqH9qnSLI"
      },
      "source": [
        "Each process has a different expected amount of events, which makes it hard to compare the shape, so we artificially modify the weights to get histograms with similar area:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OpoyM70-ngwo",
      "metadata": {
        "id": "OpoyM70-ngwo"
      },
      "outputs": [],
      "source": [
        "df_sig = dataframes['T2DegStop_250_220.root'].sample(n=6000)\n",
        "df_back = dataframes['WJets'].sample(n=12000)\n",
        "\n",
        "df_sig[\"weight\"] = df_sig[\"weight\"]*(df_back[\"weight\"].sum())/(df_sig[\"weight\"].sum())\n",
        "df = pandas.concat([df_sig, df_back])\n",
        "fig = px.histogram(\n",
        "    df,\n",
        "    x='Jet1Pt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Leading Jet Transverse Momentum - WJets vs Signal',\n",
        "    labels={'Jet1Pt': 'Jet 1 $p_T$ [GeV]'},\n",
        "    color=\"subcategory\",\n",
        "    barmode=\"overlay\", # Without this option, plotly will stack the histograms by default\n",
        "    #marginal=\"rug\",\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=800.0,\n",
        "        size=4\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XWCGwMnUpLea",
      "metadata": {
        "id": "XWCGwMnUpLea"
      },
      "outputs": [],
      "source": [
        "# Some clean up\n",
        "del df\n",
        "del df_sig\n",
        "del df_back"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MLYwbSnKryof",
      "metadata": {
        "id": "MLYwbSnKryof"
      },
      "source": [
        "We should compare the different signal points against each other to verify our expectation that the events kinematics depend to first order on the $\\Delta M$ quantity. Let check some properties within a $\\Delta M$ region:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612dE25hyCVG",
      "metadata": {
        "id": "612dE25hyCVG"
      },
      "outputs": [],
      "source": [
        "# Build dataframe for some signals points with same DM\n",
        "df_1 = dataframes['T2DegStop_250_220.root'].copy(deep=True)\n",
        "df_1['weight'] = df_1['weight']/(df_1['weight'].sum())\n",
        "df_2 = dataframes['T2DegStop_300_270.root'].copy(deep=True)\n",
        "df_2['weight'] = df_2['weight']/(df_2['weight'].sum())\n",
        "df_3 = dataframes['T2DegStop_500_470.root'].copy(deep=True)\n",
        "df_3['weight'] = df_3['weight']/(df_3['weight'].sum())\n",
        "df_4 = dataframes['T2DegStop_650_620.root'].copy(deep=True)\n",
        "df_4['weight'] = df_4['weight']/(df_4['weight'].sum())\n",
        "df_5 = dataframes['T2DegStop_800_770.root'].copy(deep=True)\n",
        "df_5['weight'] = df_5['weight']/(df_5['weight'].sum())\n",
        "df = pandas.concat([\n",
        "    df_1,\n",
        "    df_2,\n",
        "    df_3,\n",
        "    df_4,\n",
        "    df_5,\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7BPXDDjBr-cm",
      "metadata": {
        "id": "7BPXDDjBr-cm"
      },
      "outputs": [],
      "source": [
        "# Plot lep PT\n",
        "fig = px.histogram(\n",
        "    df.sample(n=12000),\n",
        "    x='LepPt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Leading Lepton Transverse Momentum - Different DM=30 signals',\n",
        "    labels={'LepPt': 'Lepton 1 $p_T$ [GeV]'},\n",
        "    color=\"subcategory\",\n",
        "    barmode=\"overlay\", # Without this option, plotly will stack the histograms by default\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=100.0,\n",
        "        size=1\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ySWg_p7ryNrx",
      "metadata": {
        "id": "ySWg_p7ryNrx"
      },
      "outputs": [],
      "source": [
        "# Plot mt\n",
        "fig = px.histogram(\n",
        "    df.sample(n=12000),\n",
        "    x='mt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Event transverse mass - Different DM=30 signals',\n",
        "    labels={'mt': '$m_T$'},\n",
        "    color=\"subcategory\",\n",
        "    barmode=\"overlay\", # Without this option, plotly will stack the histograms by default\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=500.0,\n",
        "        size=10\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i7bdNetByMpF",
      "metadata": {
        "id": "i7bdNetByMpF"
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "del df\n",
        "del df_1\n",
        "del df_2\n",
        "del df_3\n",
        "del df_4\n",
        "del df_5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WuyFsm_Tzvl0",
      "metadata": {
        "id": "WuyFsm_Tzvl0"
      },
      "source": [
        "Now, let's compare signal samples from different $\\Delta M$ regions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0wpY769nz2LV",
      "metadata": {
        "id": "0wpY769nz2LV"
      },
      "outputs": [],
      "source": [
        "# Build dataframe for some signals points with different DM\n",
        "df_1 = dataframes['T2DegStop_250_240.root'].copy(deep=True)\n",
        "df_1['weight'] = df_1['weight']/(df_1['weight'].sum())\n",
        "df_2 = dataframes['T2DegStop_250_220.root'].copy(deep=True)\n",
        "df_2['weight'] = df_2['weight']/(df_2['weight'].sum())\n",
        "df_3 = dataframes['T2DegStop_250_200.root'].copy(deep=True)\n",
        "df_3['weight'] = df_3['weight']/(df_3['weight'].sum())\n",
        "df_4 = dataframes['T2DegStop_250_170.root'].copy(deep=True)\n",
        "df_4['weight'] = df_4['weight']/(df_4['weight'].sum())\n",
        "df = pandas.concat([\n",
        "    df_1,\n",
        "    df_2,\n",
        "    df_3,\n",
        "    df_4,\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CVNbse90xPQl",
      "metadata": {
        "id": "CVNbse90xPQl"
      },
      "outputs": [],
      "source": [
        "# Plot lep PT\n",
        "fig = px.histogram(\n",
        "    df.sample(n=12000),\n",
        "    x='LepPt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Leading Jet Transverse Momentum - Signals with Different DM',\n",
        "    labels={'Jet1Pt': 'Jet 1 $p_T$ [GeV]'},\n",
        "    color=\"subcategory\",\n",
        "    barmode=\"overlay\", # Without this option, plotly will stack the histograms by default\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=100.0,\n",
        "        size=1\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gbllN3tS0Kp1",
      "metadata": {
        "id": "gbllN3tS0Kp1"
      },
      "outputs": [],
      "source": [
        "# Plot DR Jet1 Lep\n",
        "fig = px.histogram(\n",
        "    df.sample(n=12000),\n",
        "    x='mt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Event transverse mass - Signals with Different DM',\n",
        "    labels={'mt': '$m_T$'},\n",
        "    color=\"subcategory\",\n",
        "    barmode=\"overlay\", # Without this option, plotly will stack the histograms by default\n",
        ")\n",
        "fig.update_traces(xbins=dict( # bins used for histogram\n",
        "        start=0.0,\n",
        "        end=500.0,\n",
        "        size=10\n",
        "    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xJJkZGyk0K_5",
      "metadata": {
        "id": "xJJkZGyk0K_5"
      },
      "outputs": [],
      "source": [
        "# clean up\n",
        "del df\n",
        "del df_1\n",
        "del df_2\n",
        "del df_3\n",
        "del df_4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8349516-0fe8-45a9-a2a5-34a50aedbc98",
      "metadata": {
        "id": "c8349516-0fe8-45a9-a2a5-34a50aedbc98"
      },
      "source": [
        "### Multiple scatter plots\n",
        "\n",
        "We can also do scatter plots of variables against other variables, keep in mind that scatter plots implement one point per entry in the dataframe and so do not respect the event weighting of the MC samples, but they are stil very useful for understanding correlations in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WE0mrs4P5YAn",
      "metadata": {
        "id": "WE0mrs4P5YAn"
      },
      "outputs": [],
      "source": [
        "df = pandas.concat([dataframes['T2DegStop_250_220.root'], dataframes['ZInv'], dataframes['Diboson']])\n",
        "fig = px.scatter_matrix(\n",
        "    df.sample(n=30000),\n",
        "    dimensions=[\"LepPt\", \"LepEta\", \"Met\", \"mt\", \"Q80\", \"HT\"],\n",
        "    color=\"subcategory\"\n",
        "                        )\n",
        "fig.update_traces(diagonal_visible=False, showupperhalf=False,)\n",
        "fig.show()\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb0b6e7-9f6f-44b1-834b-e844ce377fb6",
      "metadata": {
        "id": "ceb0b6e7-9f6f-44b1-834b-e844ce377fb6"
      },
      "source": [
        "In the interactive plot above we can enable and disable the different colors to inspect them individually or together, we can also lasso select some of the scatter points in one of the plots and see how those specific points behave in the other plots. This is an extremely useful tool for exploring and understanding the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cduTv1Fq79n3",
      "metadata": {
        "id": "cduTv1Fq79n3"
      },
      "source": [
        "### Plots of Subleading variables\n",
        "\n",
        "Now let's look at one of the variables which does not belong to one of the leading objects or one of the event level quantities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78af4a17-ce6f-4a5c-ba84-da8e8a60c810",
      "metadata": {
        "id": "78af4a17-ce6f-4a5c-ba84-da8e8a60c810"
      },
      "outputs": [],
      "source": [
        "df = dataframes['WJets'].sample(n=15000)\n",
        "fig = px.histogram(\n",
        "    df,\n",
        "    x='Jet2Pt',\n",
        "    y='weight',\n",
        "    histfunc='sum',\n",
        "    title='Subleading Jet Transverse Momentum',\n",
        "    labels={'Jet2Pt': 'Jet 2 $p_T$ [GeV]'},\n",
        ")\n",
        "# Purposefully removing setting the binning\n",
        "#fig.update_traces(xbins=dict( # bins used for histogram\n",
        "#        start=0.0,\n",
        "#        end=100.0,\n",
        "#        size=1\n",
        "#    ))\n",
        "fig.update_layout(yaxis_title=\"Weight\")\n",
        "fig.show()\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66adcce2-1307-4084-bb70-6039dc3755ff",
      "metadata": {
        "id": "66adcce2-1307-4084-bb70-6039dc3755ff"
      },
      "source": [
        "What is up with the large peak at -10000?\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fea52fb-70d4-4463-979d-75a673b2f000",
      "metadata": {
        "id": "1fea52fb-70d4-4463-979d-75a673b2f000"
      },
      "source": [
        "The preselection of this analysis selects events with at least one lepton and at least one jet. Events with more objects of each will pass these criteria, but not all events will have multiple leptons and/or jets.\n",
        "\n",
        "As a result, a default \"unphysical\" value of -9999 is attributed to events where not all objects are present. this should serve as a \"pseudo-categorical\" variable for any machine learning approach. However, the magnitude of the unphysical default value may have an affect on the ML performance, if you want to evaluate what the effect of using a different default value may be, use the `DataC` data directory, which uses a significantly smaller magnitude value than -10000.\n",
        "\n",
        "We will use the approach of considering all the data, but there are other equally valid approaches, for instance we could consider only events with 1 lepton and 1 jet and eventually train additional algorithms for events with 2 leptons and 1 jet, 2 leptons and 2 jets and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wenrfmpaQIrB",
      "metadata": {
        "id": "wenrfmpaQIrB"
      },
      "source": [
        "### Correlation matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q4aIaLqnR2mY",
      "metadata": {
        "id": "q4aIaLqnR2mY"
      },
      "source": [
        "Evaluating the correlation matrix of the features allows to evaluate which ones are highly correlated and identify which ones could possibly not be included in the set of input features of the ML algorithm.\n",
        "\n",
        "What is particularly relevant in this application is the correlations in the different classes. If a pair of features are highly correlated in one class and not correlated in another, this information can be exploited by the ML algorithm to better separate the classes.\n",
        "\n",
        "We will be using the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), which only captures linear correlations, so the figure below is not a complete picture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3TcTi5Y7PXu1",
      "metadata": {
        "id": "3TcTi5Y7PXu1"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "def corrmatrix(corr, label):\n",
        "    plt.figure()\n",
        "    ax = sns.heatmap(\n",
        "        corr,\n",
        "        vmin=-1., vmax=1., center=0.,\n",
        "        cmap=sns.diverging_palette(20., 220., n=200, as_cmap=True),\n",
        "        square=True, xticklabels=True, yticklabels=True\n",
        "    )\n",
        "    ax.set_xticklabels(\n",
        "        ax.get_xticklabels(),\n",
        "        rotation=50,\n",
        "        horizontalalignment='right',\n",
        "        fontsize = 7\n",
        "    )\n",
        "    ax.set_yticklabels(\n",
        "        ax.get_yticklabels(),\n",
        "        horizontalalignment='right',\n",
        "        fontsize = 7\n",
        "    )\n",
        "\n",
        "    ax.set_title('Correlation matrix for %s events' % label)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "drop_cols = [\n",
        "    'Run',\n",
        "    'Event',\n",
        "    'LumiSec',\n",
        "    'Nevt',\n",
        "    'XS',\n",
        "    'nVert',\n",
        "    'weight',\n",
        "    'genWeight',\n",
        "    'sumGenWeight',\n",
        "    'BDT10',\n",
        "    'BDT20',\n",
        "    'BDT30',\n",
        "    'BDT40',\n",
        "    'BDT50',\n",
        "    'BDT60',\n",
        "    'BDT70',\n",
        "    'BDT80',\n",
        "    #'isTrain',\n",
        "    'category',\n",
        "    'subcategory',\n",
        "]\n",
        "\n",
        "corrmatrix(dataframes[\"DM30\"].drop(drop_cols, axis=1).corr(), 'signal')\n",
        "corrmatrix(dataframes[\"Background\"].drop(drop_cols, axis=1).corr(), 'background')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbe252a-508b-41ba-ac8d-c60638786a0f",
      "metadata": {
        "id": "2bbe252a-508b-41ba-ac8d-c60638786a0f"
      },
      "source": [
        "## Prepare data for ML\n",
        "\n",
        "For training the ML algorithm we will not use all the MC background processes, but only a selection of the main processes. This includes $t\\bar{t}$ and WJets processes. There are two separate simulated MC samples for $t\\bar{t}$ and the ZInv MC sample can also optionally be included. This gives a few different options for choosing on what to train the model, if you want to try a different option, scroll up and change the relevant line in the load dataframe function.\n",
        "\n",
        "We will also have to create a label for signal or background events, which will serve as the target for the ML algorithm and join the datafraes together. We will also drop a number of the features, keeping only those which we want the ML algorithm to train on. In order to maintain parity with the paper, we will use by default the same set as there, but please consider trying different features to see if you are able to improve upon the paper results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82abb192-3784-48c5-b829-7a288f4e89f4",
      "metadata": {
        "id": "82abb192-3784-48c5-b829-7a288f4e89f4"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'label' and set its value to 1 or 0 for all rows (=events)\n",
        "sig = dataframes['DM30'].copy(deep=True)\n",
        "bkg = dataframes['Background'].copy(deep=True)\n",
        "sig['label'] = 1\n",
        "bkg['label'] = 0\n",
        "\n",
        "# Merge the signal and background into one dataframe\n",
        "print(f\"Signal shape {sig.shape}\")\n",
        "print(f\"Bkg shape {bkg.shape}\")\n",
        "\n",
        "data = pandas.concat([sig,bkg])\n",
        "\n",
        "print(f\"Data shape {data.shape}\")\n",
        "print(data.columns)\n",
        "\n",
        "# Drop unneeded features, If you are using a different DM region, keep the BDT of your DM here instead of BDT30\n",
        "data = data[[\"Jet1Pt\",\"mt\",\"Met\",\"LepChg\",\"LepEta\",\"LepPt\",\"HT\",\"NbLoose\",\"Njet\",\"JetHBpt\",\"DrJetHBLep\",\"JetHBCSV\",\"BDT30\",\"weight\",\"label\"]]\n",
        "\n",
        "print(f\" Data shape {data.shape}\")\n",
        "print(data.columns)\n",
        "print(f\"In this dataframe we finally have {data[data['label']==1].shape[0]} signal and {data[data['label']==0].shape[0]} background events\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68295cd-910b-4863-9af2-074fb604abce",
      "metadata": {
        "id": "f68295cd-910b-4863-9af2-074fb604abce"
      },
      "source": [
        "This data set is still ordered, ie. each signal point comes after each other, followed by the background events, first $t\\bar{t}$ and then WJets. ML training requires a shuffled data set instead.\n",
        "\n",
        "It is good practice to shuffle the data as soon as possible even though shuffling can be done at the time of splitting into the training and testing datasets.\n",
        "\n",
        "We will also separate features and labels from each other, and check for corrupted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jd1gyHaOGFWF",
      "metadata": {
        "id": "jd1gyHaOGFWF"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ea526c-e4f1-47a1-89e7-1734df683361",
      "metadata": {
        "id": "17ea526c-e4f1-47a1-89e7-1734df683361"
      },
      "outputs": [],
      "source": [
        "print(\"There are NaN-filled elements:\", data.isna().any().any())\n",
        "\n",
        "X = data.drop([\"label\"], axis=1)\n",
        "y = data[\"label\"]\n",
        "\n",
        "print(f\"data shape {data.shape}\")\n",
        "print(f\"input feature shape {X.shape}\")\n",
        "print(f\"label (=target) shape {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaa22604-2037-4038-a5cc-275403e331cc",
      "metadata": {
        "id": "eaa22604-2037-4038-a5cc-275403e331cc"
      },
      "source": [
        "### Split the data set into training and test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b41324f-fe1c-4ad1-adee-e4fb7cbd3745",
      "metadata": {
        "id": "9b41324f-fe1c-4ad1-adee-e4fb7cbd3745"
      },
      "source": [
        "Typically, in a ML approach, there should be 3 independent datasets, a training dataset on which the ML algorithm is trained; a testing dataset on which the ML algorithm is tested and whose results are used to inform on hyperparameter tuning and other optimizations; and an application dataset, which is the data we are really interested in studying.\n",
        "\n",
        "Often, the test and application datasets are merged, particularly when there is no tuning of the hyperparameters, but we should always be aware of this shortcut since it may lead to biases. This is what we will do here.\n",
        "\n",
        "For full compatibility with the paper results, we should also use the same splitting as was used in the paper, unfortunately, this data has been challenging to recover, so we will define our own splitting and hope that the results obtained here still translate and compare favourably to those of the paper.\n",
        "\n",
        "![Example sample splitting](figs/trainingNetwork.png)\n",
        "\n",
        "(Image: P. Vischia, [doi:10.5281/zenodo.6373442](https://doi.org/10.5281/zenodo.6373442))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfff38ff-768f-47b0-9091-8a5440c73522",
      "metadata": {
        "id": "bfff38ff-768f-47b0-9091-8a5440c73522"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "print(f\"We have {len(X_train)} training samples with {sum(y_train)} signal and {sum(1-y_train)} background events\")\n",
        "print(f\"We have {len(X_test)} testing samples with {sum(y_test)} signal and {sum(1-y_test)} background events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HgtbHZ_SZ3k_",
      "metadata": {
        "id": "HgtbHZ_SZ3k_"
      },
      "source": [
        "So far we have purposefully kept the BDT output, from the paper, attached as a feature of the data as well as the event weight. We should now remove it to avoid feeding it into the ML algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aP0yba3XZypi",
      "metadata": {
        "id": "aP0yba3XZypi"
      },
      "outputs": [],
      "source": [
        "BDT_train = X_train[\"BDT30\"]\n",
        "BDT_test = X_test[\"BDT30\"]\n",
        "weight_train = X_train[\"weight\"]\n",
        "weight_test = X_test[\"weight\"]\n",
        "\n",
        "X_train.drop([\"BDT30\", \"weight\"], axis=1, inplace=True)\n",
        "X_test.drop([\"BDT30\", \"weight\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6554c436-6697-4725-bccd-531b38c3104a",
      "metadata": {
        "id": "6554c436-6697-4725-bccd-531b38c3104a"
      },
      "source": [
        "### Preprocess the data\n",
        "Before training a first ML-based classifier we need to think about if any preprocessing of the data is required. Many ML algorithms are based on gradient minimization techniques that can fail if the inputs have numbers that widely-vary in magnitude (remember the default \"unphysical\" value of -9999). For example, we saw the leading Jet $p_T$ covers a range of several orders of magnitude, which can prevent the convergence of a minimization algorithm.\n",
        "\n",
        "There are several scaling methods offered in the ML libraries, each method has its own advantages and drawbacks. Another possibility is to use PCA, which can result in a more targetted set of input features. Below we use the Standard Scaler by default, but import a few of the other options for you to experiment with.\n",
        "\n",
        "We will also store the original train and test structures, in case you ever need to revert.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655bf1cb-b828-4c77-b397-a98a0122c62d",
      "metadata": {
        "id": "655bf1cb-b828-4c77-b397-a98a0122c62d"
      },
      "outputs": [],
      "source": [
        "X_train_orig = X_train.copy()\n",
        "y_train_orig = y_train.copy()\n",
        "X_test_orig = X_test.copy()\n",
        "y_test_orig = y_test.copy()\n",
        "\n",
        "from sklearn.preprocessing import (\n",
        "    MaxAbsScaler, # maxAbs\n",
        "    MinMaxScaler, # MinMax\n",
        "    Normalizer, # Normalization (equal integral)\n",
        "    StandardScaler# standard scaling\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Scale the input features and the target variable\n",
        "for column in X_train.columns:\n",
        "    scaler = StandardScaler().fit(X_train.filter([column], axis=1))\n",
        "    X_train[column] = scaler.transform(X_train.filter([column], axis=1))\n",
        "    X_test[column] = scaler.transform(X_test.filter([column], axis=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563813ad-37b4-435c-9aa2-42ac9f47375c",
      "metadata": {
        "id": "563813ad-37b4-435c-9aa2-42ac9f47375c"
      },
      "source": [
        "## Train a dense neural network\n",
        "\n",
        "\n",
        "For this exercise we will use `pytorch`, a backend designed natively for tensor operations.\n",
        "\n",
        "You could also use one of the other machine learning libraries available, such as the `tensorflow` backend, either directly or through the `keras` frontend.\n",
        "\n",
        "`torch` handles the data management via the `Dataset` and `DataLoader` classes.\n",
        "Here we don't need any specific `Dataset` class, because we are not doing sophisticated things, but you may need that in the future.\n",
        "\n",
        "The `DataLoader` class takes care of providing quick access to the data by sampling batches that are then fed to the network for (mini)batch gradient descent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef78064-3c16-4112-8c9e-ec05c2138079",
      "metadata": {
        "id": "0ef78064-3c16-4112-8c9e-ec05c2138079"
      },
      "source": [
        "Set a manual seed, for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff1d948-1185-41a0-8bd1-013d1910c210",
      "metadata": {
        "id": "6ff1d948-1185-41a0-8bd1-013d1910c210"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9422ec-c810-452b-82f5-edff81dcf3c1",
      "metadata": {
        "id": "2e9422ec-c810-452b-82f5-edff81dcf3c1"
      },
      "outputs": [],
      "source": [
        "class StopDataset(Dataset):\n",
        "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
        "        self.X = torch.Tensor(X.values if isinstance(X, pandas.core.frame.DataFrame) else X).to(device)\n",
        "        self.y = torch.Tensor(y.values).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.y[idx]\n",
        "        datum = self.X[idx]\n",
        "\n",
        "        return datum, label\n",
        "\n",
        "batch_size=512 # Minibatch learning\n",
        "\n",
        "\n",
        "train_dataset = StopDataset(X_train, y_train)\n",
        "test_dataset = StopDataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f4b9160-5fe8-4ed9-98ef-dfdce2432046",
      "metadata": {
        "id": "0f4b9160-5fe8-4ed9-98ef-dfdce2432046"
      },
      "source": [
        "As an example of the DataLoader functionality, let's access the data loader via its iterator, and sample a single batch by calling `next` on the iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EaLChBibe7Oy",
      "metadata": {
        "id": "EaLChBibe7Oy"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "train_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfe9146-6213-40c9-a91d-f78341e1e826",
      "metadata": {
        "id": "fcfe9146-6213-40c9-a91d-f78341e1e826"
      },
      "source": [
        "Let's build a simple neural network, by inheriting from the `nn.Module` class. **This is very crucial, because that class is the responsible for providing the automatic differentiation infrastructure for tracking parameters and performing backpropagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "334a1cac-2fe6-4d02-b899-b4ba8b707161",
      "metadata": {
        "id": "334a1cac-2fe6-4d02-b899-b4ba8b707161"
      },
      "outputs": [],
      "source": [
        "class StopNeuralNetwork(nn.Module):\n",
        "    def __init__(self, ninputs, device=torch.device(\"cpu\")):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(ninputs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.linear_relu_stack.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass data through conv1\n",
        "        x = self.linear_relu_stack(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1ee743-cc12-4a5e-8e36-65f809b634a6",
      "metadata": {
        "id": "9e1ee743-cc12-4a5e-8e36-65f809b634a6"
      },
      "source": [
        "Let's instantiate the neural network and print some info on it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef615813-e4c5-403e-8488-7008cdce36ee",
      "metadata": {
        "id": "ef615813-e4c5-403e-8488-7008cdce36ee"
      },
      "outputs": [],
      "source": [
        "model = StopNeuralNetwork(X_train.shape[1])\n",
        "\n",
        "print(model) # some basic info\n",
        "\n",
        "print(\"Now let's see some more detailed info by using the torchinfo package\")\n",
        "torchinfo.summary(model, input_size=(batch_size, X_train.shape[1])) # the input size is (batch size, number of features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39cf99c-1c0a-4679-ba59-c7a3b3f9a820",
      "metadata": {
        "id": "f39cf99c-1c0a-4679-ba59-c7a3b3f9a820"
      },
      "outputs": [],
      "source": [
        "train_dataset = StopDataset(X_train, y_train, device=device)\n",
        "test_dataset = StopDataset(X_test, y_test, device=device)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Reinstantiate the model, on the chosen device\n",
        "model = StopNeuralNetwork(X_train.shape[1], device)\n",
        "\n",
        "#check if the NN can be evaluated some data; note: it has not been trained yet\n",
        "print (model(torch.tensor(X_train.values[:10],device=device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c60e984-c72d-42a3-b04d-73a6b2204af8",
      "metadata": {
        "id": "4c60e984-c72d-42a3-b04d-73a6b2204af8"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, best_model_path, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    losses=[] # Track the loss function\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    model.train()\n",
        "    best_loss = np.inf\n",
        "    for (X,y) in tqdm(dataloader):\n",
        "        # Reset gradients (to avoid their accumulation)\n",
        "        optimizer.zero_grad()\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred.squeeze(dim=1), y)\n",
        "        losses.append(loss.detach().cpu())\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss.detach().cpu()\n",
        "            torch.save(model.state_dict(), best_model_path) # Save the full state of the model, to have access to the training history\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d98e03ac-147b-4895-85c0-1a77d5cbe2df",
      "metadata": {
        "id": "d98e03ac-147b-4895-85c0-1a77d5cbe2df"
      },
      "source": [
        "Now we need to define the loop that is run on the test dataset.\n",
        "\n",
        "**The test dataset is just used for evaluating the output of the model. No backpropagation is needed, therefore backpropagation must be switched off!!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26311c24-455f-41b7-a5b6-0563071dad0a",
      "metadata": {
        "id": "26311c24-455f-41b7-a5b6-0563071dad0a"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn, device):\n",
        "    losses=[] # Track the loss function\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for (X,y) in tqdm(dataloader):\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred.squeeze(dim=1), y).item()\n",
        "            losses.append(loss)\n",
        "\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883c0895-2ffd-4543-aa29-f40740de3241",
      "metadata": {
        "id": "883c0895-2ffd-4543-aa29-f40740de3241"
      },
      "source": [
        "We are now ready to train this network!\n",
        "\n",
        "Torch provides the functionality to use generic functions as loss function. Since we are trying to do classification, we will set our loss function to be the cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecfe6e4-082f-4f26-a70b-59d8dea0eae8",
      "metadata": {
        "id": "4ecfe6e4-082f-4f26-a70b-59d8dea0eae8"
      },
      "outputs": [],
      "source": [
        "epochs=30\n",
        "learningRate = 0.01\n",
        "\n",
        "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "# The optimizer decides which path to follow through the gradient of the loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "# The scheduler reduces the learning rate for the optimizer in order for the optimizer to be able to \"enter\" narrow minima\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e19eec4-bf34-440b-ac76-14599477ae6a",
      "metadata": {
        "id": "5e19eec4-bf34-440b-ac76-14599477ae6a"
      },
      "outputs": [],
      "source": [
        "train_losses=[]\n",
        "test_losses=[]\n",
        "best_model_path = \"best_stop_dnn_model.h5\"\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
        "    train_loss=train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, best_model_path, device)\n",
        "    test_loss=test_loop(test_dataloader, model, loss_fn, device)\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gihj-qVUnH13",
      "metadata": {
        "id": "gihj-qVUnH13"
      },
      "source": [
        "We can now plot the loss evolution over the training epochs to monitor the training progression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c152d78-62b0-43f0-b64d-fcbabd238661",
      "metadata": {
        "id": "9c152d78-62b0-43f0-b64d-fcbabd238661"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Average training loss\")\n",
        "plt.plot(test_losses, label=\"Average test loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xpcOdUITngAp",
      "metadata": {
        "id": "xpcOdUITngAp"
      },
      "source": [
        "We can now plot the Receiver Operating Characteristic (ROC) curve of the Neural network model to get an evaluation of the performance of this approach. We can also use the BDT data from the paper to print the ROC curve of the BDT approach to get a preliminary comparison between the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b53d07-f43d-469a-9b99-c296e12cdc60",
      "metadata": {
        "id": "29b53d07-f43d-469a-9b99-c296e12cdc60"
      },
      "outputs": [],
      "source": [
        "def plot_rocs(scores_labels_names):\n",
        "    plt.figure()\n",
        "    for score, label, weights, name  in scores_labels_names:\n",
        "        fpr, tpr, thresholds = roc_curve(label, score, sample_weight=weights)\n",
        "        plt.plot(\n",
        "            fpr, tpr,\n",
        "            linewidth=2,\n",
        "            label=f\"{name} (AUC = {100.*auc(fpr, tpr): .2f} %)\"\n",
        "        )\n",
        "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "    plt.grid()\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "with torch.no_grad():\n",
        "  NN_train = model(torch.tensor(X_train.to_numpy(),device=model.device)).numpy(force=True)\n",
        "  NN_test = model(torch.tensor(X_test.to_numpy(),device=model.device)).numpy(force=True)\n",
        "plot_rocs([\n",
        "    (NN_train, y_train, weight_train, \"Train\"),\n",
        "    (NN_test, y_test, weight_test, \"Test\"),\n",
        "    (BDT_test, y_test, weight_test, \"BDT\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Uocn0lloeSs",
      "metadata": {
        "id": "3Uocn0lloeSs"
      },
      "source": [
        "The results above seem to indicate that the neural network approach can bring some improvement to the BDT approach used in the paper.\n",
        "\n",
        "A more rigorous evaluation should be performed, where a final selection is decided upon and the expected limits are computed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v1-MwijszCOk",
      "metadata": {
        "id": "v1-MwijszCOk"
      },
      "source": [
        "### Model Evaluation with pyhf\n",
        "\n",
        "We will evaluate the model performance with pyhf and compare the BDT approach from the paper with the NN approach used here. We will use some simplifications, such as ignoring all background processes not in the training background sample (we leave it as an exercise to implement the other background samples to any interested individual, the data has been loaded and is available, you just need to apply the trained NN to the required samples). We will also assume an integrated luminsoty of $35.866\\,fb^{-1}$ for a more direct comparison to the results from the paper. We also assume that of the 23 signal samples contributing to the $\\Delta M$ sample that they all contribute equally, i.e. we average the cross sections across the samples. This is for ease of computation and because we need to choose a single reference cross section for performing the calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oLO-Jw3Vz2Z6",
      "metadata": {
        "id": "oLO-Jw3Vz2Z6"
      },
      "outputs": [],
      "source": [
        "import pyhf\n",
        "pyhf.set_backend(\"numpy\")\n",
        "from pyhf.contrib.viz import brazil\n",
        "\n",
        "luminosity = 35866"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "erazu0fs99Im",
      "metadata": {
        "id": "erazu0fs99Im"
      },
      "source": [
        "#### BDT Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kCsXp4elyNCj",
      "metadata": {
        "id": "kCsXp4elyNCj"
      },
      "outputs": [],
      "source": [
        "# BDT final selection\n",
        "sig_yield = weight_test.loc[(BDT_test > 0.47) & (y_test == 1)].sum() * luminosity/(0.33*23)  #Divide by 0.33 to compensate for the splitting factor\n",
        "bkg_yield = weight_test.loc[(BDT_test > 0.47) & (y_test == 0)].sum() * luminosity/0.33\n",
        "# cuts (adjust to the signal DM you used):\n",
        "#  DM10 : 0.31\n",
        "#  DM20 : 0.39\n",
        "#  DM30 : 0.47\n",
        "#  DM40 : 0.48\n",
        "#  DM50 : 0.45\n",
        "#  DM60 : 0.50\n",
        "#  DM70 : 0.46\n",
        "#  DM80 : 0.44\n",
        "\n",
        "print(f\"Signal yield: {sig_yield}\")\n",
        "print(f\"Background yield: {bkg_yield}\")\n",
        "\n",
        "\n",
        "stat_model = pyhf.simplemodels.uncorrelated_background(\n",
        "    signal=[sig_yield], bkg=[bkg_yield], bkg_uncertainty=[bkg_yield*0.2] # Assume 20% uncertainty on background\n",
        ")\n",
        "data = [bkg_yield] + stat_model.config.auxdata\n",
        "\n",
        "poi_vals = np.linspace(0, 3, 31)\n",
        "results = [\n",
        "    pyhf.infer.hypotest(\n",
        "        test_poi, data, stat_model, test_stat=\"qtilde\", return_expected_set=True\n",
        "    )\n",
        "    for test_poi in poi_vals\n",
        "]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(7, 5)\n",
        "brazil.plot_results(poi_vals, results, ax=ax)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mZXSEUea-Aiv",
      "metadata": {
        "id": "mZXSEUea-Aiv"
      },
      "source": [
        "#### NN Approach\n",
        "\n",
        "For the NN approach, we need to choose a cut value, select the cut value which maximizes FOM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the numpy array into pandas (or else the memory consumption will explode) and also keep the indexing correct\n",
        "test_df = pandas.DataFrame({'index': y_test.reset_index()['index'],'NN': NN_test[:, 0]})\n",
        "test_df.set_index('index', inplace=True)\n",
        "NN_test = test_df['NN']"
      ],
      "metadata": {
        "id": "2HktHQ3EmDqk"
      },
      "id": "2HktHQ3EmDqk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yc89aTfiA6-J",
      "metadata": {
        "id": "yc89aTfiA6-J"
      },
      "outputs": [],
      "source": [
        "# Maximize FOM, for now use simple S/sqrt(B)\n",
        "from math import sqrt\n",
        "from operator import itemgetter\n",
        "\n",
        "cut_vals = np.linspace(0.0, 1.0, 201)\n",
        "cut_vals = cut_vals[:-1]\n",
        "#print(cut_vals)\n",
        "FOM_vals = []\n",
        "\n",
        "for cut in cut_vals:\n",
        "  S = weight_test.loc[(NN_test > cut) & (y_test == 1)].sum() * (luminosity/(0.33*23))\n",
        "  B = weight_test.loc[(NN_test > cut) & (y_test == 0)].sum() * (luminosity/0.33)\n",
        "  FOM = S/sqrt(B)\n",
        "  FOM_vals.append((cut, FOM))\n",
        "\n",
        "print(max(FOM_vals, key=itemgetter(1)))\n",
        "NN_cut = max(FOM_vals, key=itemgetter(1))[0]\n",
        "NN_cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "krrwyCmL-AWD",
      "metadata": {
        "id": "krrwyCmL-AWD"
      },
      "outputs": [],
      "source": [
        "# BDT final selection\n",
        "\n",
        "sig_yield = weight_test.loc[(NN_test > NN_cut) & (y_test == 1)].sum() * luminosity/(0.33*23)  #Divide by 0.33 to compensate for the splitting factor\n",
        "bkg_yield = weight_test.loc[(NN_test > NN_cut) & (y_test == 0)].sum() * luminosity/0.33\n",
        "\n",
        "print(f\"Signal yield: {sig_yield}\")\n",
        "print(f\"Background yield: {bkg_yield}\")\n",
        "\n",
        "stat_model = pyhf.simplemodels.uncorrelated_background(\n",
        "    signal=[sig_yield], bkg=[bkg_yield], bkg_uncertainty=[bkg_yield*0.2] # Assume 20% uncertainty on background\n",
        ")\n",
        "data = [bkg_yield] + model.config.auxdata\n",
        "\n",
        "poi_vals = np.linspace(0, 3, 31)\n",
        "results = [\n",
        "    pyhf.infer.hypotest(\n",
        "        test_poi, data, model, test_stat=\"qtilde\", return_expected_set=True\n",
        "    )\n",
        "    for test_poi in poi_vals\n",
        "]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(7, 5)\n",
        "brazil.plot_results(poi_vals, results, ax=ax)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a3314d-ec3c-48e2-a583-00244d7e95c1",
      "metadata": {
        "id": "46a3314d-ec3c-48e2-a583-00244d7e95c1"
      },
      "source": [
        "## Train an autoencoder neural network\n",
        "\n",
        "An autoencoder is a neural network design which is trained to provide the same output as its input. However, the internal layers go through a \"bottleneck\", forcing the NN to learn an internal representation of the data. As a result, an autoencoder structure is usually thought of as first an encoder, which encodes the data into a simpler representation followed by a decoder which reconstructs the original data from the encoded representation.\n",
        "\n",
        "![Autoencoder](figs/Autoencoder_structure.png)\n",
        "\n",
        "(Image: Wikimedia Commons, https://commons.wikimedia.org/wiki/File:Autoencoder_structure.png)\n",
        "\n",
        "Autoencoders have been used for various purposes, in the context of this exercise we plan to use it as an anomaly detector. The underlying idea of using an autoencoder for anomaly detection is that by training the autoencoder only on background simulated events, when it is presented with signal events, whether of our specific model of interest or any other model, the encoder and decoder portions would not operate correctly for these events, thus the output would not match the input. This would allow to appropriately tag these anomalous events. However, this is an assumed property of an autoencoder and there has been some literature showing cases where this property does not hold true [https://arxiv.org/abs/1810.09136].\n",
        "\n",
        "We will follow a similar structure to above, for the deep neural network, so not much more explanation will be added.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NYUKri_aN8-T",
      "metadata": {
        "id": "NYUKri_aN8-T"
      },
      "source": [
        "Prepare the data with only the background processes (not training on signal):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sGn-N9fhN-3P",
      "metadata": {
        "id": "sGn-N9fhN-3P"
      },
      "outputs": [],
      "source": [
        "X_train_bck = X_train.loc[y_train == 0]\n",
        "X_test_bck = X_test.loc[y_test == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WkQcKdsV-yhM",
      "metadata": {
        "id": "WkQcKdsV-yhM"
      },
      "source": [
        "Create the data handlers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RM2w79Mc-xak",
      "metadata": {
        "id": "RM2w79Mc-xak"
      },
      "outputs": [],
      "source": [
        "class StopAutoencoderDataset(Dataset):\n",
        "    def __init__(self, X, device=torch.device(\"cpu\")):\n",
        "        self.X = torch.Tensor(X.values if isinstance(X, pandas.core.frame.DataFrame) else X).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx]\n",
        "\n",
        "ae_batch_size=512 # Minibatch learning\n",
        "\n",
        "\n",
        "ae_train_dataset = StopAutoencoderDataset(X_train_bck, device=device)\n",
        "ae_test_dataset = StopAutoencoderDataset(X_test_bck, device=device)\n",
        "\n",
        "ae_train_dataloader = DataLoader(ae_train_dataset, batch_size=ae_batch_size, shuffle=True)\n",
        "ae_test_dataloader = DataLoader(ae_test_dataset, batch_size=ae_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fgULBEaO_Xcy",
      "metadata": {
        "id": "fgULBEaO_Xcy"
      },
      "source": [
        "Create the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62d4d19-62ca-4f05-8871-06ff3f10d699",
      "metadata": {
        "id": "b62d4d19-62ca-4f05-8871-06ff3f10d699"
      },
      "outputs": [],
      "source": [
        "class StopAutoEncoder(torch.nn.Module):\n",
        "    def __init__(self, ninputs, latent_dim=8, device=torch.device(\"cpu\")):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(ninputs, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 36),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(36, 18),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(18, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Build the decoder\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dim, 18),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(18, 36),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(36, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, ninputs),\n",
        "            #torch.nn.Sigmoid() # Can not be use in our case since the input features are scaled with Standard scalar and may have values in nearly any range\n",
        "            # If using the MinMax scaler, where the input is bounded from 0 to 1, then sigmoid could be considered\n",
        "            #torch.nn.Tanh() # for inputs bounded between -1 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dOZfD3r7_Zi5",
      "metadata": {
        "id": "dOZfD3r7_Zi5"
      },
      "source": [
        "Inspect the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xN9LRoKk_Z6E",
      "metadata": {
        "id": "xN9LRoKk_Z6E"
      },
      "outputs": [],
      "source": [
        "ae_model = StopAutoEncoder(X_train.shape[1], device=device)\n",
        "\n",
        "print(ae_model) # some basic info\n",
        "\n",
        "print(\"Now let's see some more detailed info by using the torchinfo package\")\n",
        "torchinfo.summary(ae_model, input_size=(ae_batch_size, X_train.shape[1])) # the input size is (batch size, number of features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oAy__VWPARGf",
      "metadata": {
        "id": "oAy__VWPARGf"
      },
      "outputs": [],
      "source": [
        "#check if the NN can be evaluated some data; note: it has not been trained yet\n",
        "print (ae_model(torch.tensor(X_train.values[:10],device=device)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EjBHoj0rIz3N",
      "metadata": {
        "id": "EjBHoj0rIz3N"
      },
      "source": [
        "Define the train loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iMOyFbhSIzk0",
      "metadata": {
        "id": "iMOyFbhSIzk0"
      },
      "outputs": [],
      "source": [
        "def ae_train_loop(dataloader, model, loss_fn, optimizer, scheduler, best_model_path, device):\n",
        "    size = len(dataloader.dataset)\n",
        "    losses=[] # Track the loss function\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    model.train()\n",
        "    best_loss = np.inf\n",
        "    for X in tqdm(dataloader):\n",
        "        # Reset gradients (to avoid their accumulation)\n",
        "        optimizer.zero_grad()\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, X)\n",
        "        losses.append(loss.detach().cpu())\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss.detach().cpu()\n",
        "            torch.save(model.state_dict(), best_model_path) # Save the full state of the model, to have access to the training history\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "    return np.mean(losses), np.std(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb_a6eOJvpl",
      "metadata": {
        "id": "9fb_a6eOJvpl"
      },
      "source": [
        "Define the test loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M_I0FDDBJvaR",
      "metadata": {
        "id": "M_I0FDDBJvaR"
      },
      "outputs": [],
      "source": [
        "def ae_test_loop(dataloader, model, loss_fn, device):\n",
        "    losses=[] # Track the loss function\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X in tqdm(dataloader):\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, X).item()\n",
        "            losses.append(loss)\n",
        "\n",
        "    return np.mean(losses), np.std(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l_wcPz6pK4_8",
      "metadata": {
        "id": "l_wcPz6pK4_8"
      },
      "source": [
        "Define additional parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xnbvUlCLK4tz",
      "metadata": {
        "id": "xnbvUlCLK4tz"
      },
      "outputs": [],
      "source": [
        "epochs=40\n",
        "learningRate = 0.05\n",
        "\n",
        "# The loss defines the metric deciding how good or bad is the prediction of the network\n",
        "ae_loss_fn = torch.nn.MSELoss()\n",
        "# The optimizer decides which path to follow through the gradient of the loss function\n",
        "ae_optimizer = torch.optim.SGD(ae_model.parameters(), lr=learningRate)\n",
        "# The scheduler reduces the learning rate for the optimizer in order for the optimizer to be able to \"enter\" narrow minima\n",
        "ae_scheduler = torch.optim.lr_scheduler.ExponentialLR(ae_optimizer, gamma=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRqqGphaNIv0",
      "metadata": {
        "id": "LRqqGphaNIv0"
      },
      "source": [
        "Train the Autoencoder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EQsA-jEHNJXO",
      "metadata": {
        "id": "EQsA-jEHNJXO"
      },
      "outputs": [],
      "source": [
        "train_losses=[]\n",
        "train_losses_std=[]\n",
        "test_losses=[]\n",
        "test_losses_std=[]\n",
        "best_model_path = \"best_stop_ae_model.h5\"\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
        "    train_loss, train_loss_std=ae_train_loop(ae_train_dataloader, ae_model, ae_loss_fn, ae_optimizer, ae_scheduler, best_model_path, device)\n",
        "    test_loss, test_loss_std=ae_test_loop(ae_test_dataloader, ae_model, ae_loss_fn, device)\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    train_losses_std.append(train_loss_std)\n",
        "    test_losses_std.append(test_loss_std)\n",
        "    print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", ae_scheduler.get_last_lr())\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nlYjqGzlNxNn",
      "metadata": {
        "id": "nlYjqGzlNxNn"
      },
      "source": [
        "Plot the loss evolution over training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Gx3BxhiNzQx",
      "metadata": {
        "id": "2Gx3BxhiNzQx"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(train_losses, label=\"Average training loss\")\n",
        "plt.plot(test_losses, label=\"Average test loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U8qIWaYeS63Q",
      "metadata": {
        "id": "U8qIWaYeS63Q"
      },
      "source": [
        "To get the ROC curve, we need to evaluate the autoencoder on a dataset which contains signal events. We also need a way to identify outliers. The loss function we have been using to optimize the neural network provides a convenient and quick way to evaluate how much the output differs from the input, and by using the loss information from the training we are able to define an expected mean and standard deviation for the loss, thus we can now define a criteria to identify outliers and ultimately plot the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WSdRqRjMS5z1",
      "metadata": {
        "id": "WSdRqRjMS5z1"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  X_test_tensor = torch.tensor(X_test.to_numpy(),device=ae_model.device)\n",
        "  prediction = ae_model(X_test_tensor)\n",
        "  loss = torch.mean((prediction - X_test_tensor)**2, dim=1).numpy(force=True)\n",
        "  ae_test = (loss - test_losses[-1])/test_losses_std[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UV7toPZMUlmv",
      "metadata": {
        "id": "UV7toPZMUlmv"
      },
      "outputs": [],
      "source": [
        "def plot_rocs(scores_labels_names):\n",
        "    plt.figure()\n",
        "    for score, label, weights, name  in scores_labels_names:\n",
        "        fpr, tpr, thresholds = roc_curve(label, score, sample_weight=weights)\n",
        "        plt.plot(\n",
        "            fpr, tpr,\n",
        "            linewidth=2,\n",
        "            label=f\"{name} (AUC = {100.*auc(fpr, tpr): .2f} %)\"\n",
        "        )\n",
        "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "    plt.grid()\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"Receiver Operating Characteristic curve\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "plot_rocs([\n",
        "    (ae_test, y_test, weight_test, \"AE\"),\n",
        "    (NN_test, y_test, weight_test, \"DNN\"),\n",
        "    (BDT_test, y_test, weight_test, \"BDT\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CmU_o2_8Y2L2",
      "metadata": {
        "id": "CmU_o2_8Y2L2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9657393f-2cf4-40a6-be37-d9f4996ba79d",
      "metadata": {
        "id": "9657393f-2cf4-40a6-be37-d9f4996ba79d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v1-MwijszCOk"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}