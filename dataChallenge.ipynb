{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc8d289-3918-452e-b6e1-28ed97caa8e4",
   "metadata": {},
   "source": [
    "# Lisbon Machine Learning School\n",
    "## Exercise 3: data preprocessing, and neural network structure\n",
    "\n",
    "(C) Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49ac6-f688-46d1-82bb-c93ef1eb2cb6",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "- If you are running locally, you don't need to run anything\n",
    "\n",
    "- If you are running on Google Colab, uncomment and run the next cell (remove only the \"#\", keep the \"!\"). You can also run it from a local installation, but it will do nothing if you have already installed all dependencies (and it will take some time to tell you it is not gonna do anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9fe7a-5c11-4ce6-be45-402a10f052a0",
   "metadata": {},
   "source": [
    "## Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894a5d9-5ab1-442b-af11-13bcf3f79f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchinfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "import uproot\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "print('Using torch version', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b52b21-e0e5-4c2a-9afb-e42afb99e42f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cf761-a0b2-496c-83bc-5344dca6902d",
   "metadata": {},
   "source": [
    "We will use the same data we used for exercise 2, that is simulated events corresponding to three physics processes.\n",
    "- ttH production\n",
    "- ttW production\n",
    "- Drell-Yan ($pp\\\\to Z/\\\\gamma^*$+jets) production\n",
    "\n",
    "We will select the multilepton final state, which is a challenging final state with a rich structure and nontrivial background separation.\n",
    "\n",
    "<img src=\"figs/2lss.png\" alt=\"ttH multilepton 2lss\" style=\"width:40%\"/>\n",
    "\n",
    "We use the [uproot](https://uproot.readthedocs.io/en/latest/basic.html) library to conveniently read in a [ROOT TNuple](https://root.cern.ch/doc/master/classTNtuple.html) which can automatically convert it to a [pandas dataframe](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a60566-fd81-4c4b-b80c-1dafc8dc60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data only if you haven't done so yet\n",
    "\n",
    "if not os.path.isfile(\"data/signal_blind20.root\"): \n",
    "    !mkdir data; cd data/; wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz; tar xzvf lisbon_ml_school_tth.tar.gz; rm lisbon_ml_school_tth.tar.gz; cd -;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829b510-e956-477d-a841-81b0a369ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = './data'\n",
    "\n",
    "sig = uproot.open(os.path.join(INPUT_FOLDER,'signal_blind20.root'))['Friends'].arrays(library=\"pd\")\n",
    "bk1 = uproot.open(os.path.join(INPUT_FOLDER,'background_1.root'))['Friends'].arrays(library=\"pd\")\n",
    "bk2 = uproot.open(os.path.join(INPUT_FOLDER,'background_2.root'))['Friends'].arrays(library=\"pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc7ae9-0d8e-4dac-b3bc-b8e986eb1654",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Select the features you want to use for this exercise, don't forget to remove unnecessary features.\n",
    "\n",
    "Most of the variables are input features, corresponding to detector measurements of the properties of the reconstructed decay products.\n",
    "\n",
    "There are three special variables, though:\n",
    "\n",
    "- `Hreco_evt_tag`: this feature has values in ${0,1}$, where $1$ flags the event as signal event, and $0$ flags the event as background event;\n",
    "- `Hreco_HTXS_Higgs_pt`: this feature contains the true generate Higgs boson transverse momentum at generator level (used for regression);\n",
    "- `Hreco_HTXS_Higgs_y`: this feature contains the true generated Higgs boson rapidity (not pseudorapidity) at generator level (used for regression).\n",
    "\n",
    "\n",
    "### Important\n",
    "\n",
    "Twenty percent of the events have `-99` in the `Hreco_HTXS_Higgs_pt` and `Hreco_HTXS_Higgs_y` values. These are the \"unlabelled\" events that you will have to send predictions for. You should filter them out for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dec0a6-c8da-4201-b7a9-675be0612864",
   "metadata": {},
   "source": [
    "## The assignment\n",
    "\n",
    "- For this data challenge, your target is to simultaneously regress the Higgs transverse momentum `Hreco_HTXS_Higgs_pt` and the rapidity `Hreco_HTXS_Higgs_y`, in `2lss` events. As a reminder, this means filtering the data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b6ce3-0f17-4d42-a581-1ad77a019490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "data=data[data['Hreco_Lep2_pt']==-99]\n",
    "# Drop unneeded features\n",
    "data = data.drop([\"Hreco_Lep2_pt\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \n",
    "                  \"Hreco_evt_tag\",\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"], axis=1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a72e29-98fb-41ac-b178-30402b03968a",
   "metadata": {},
   "source": [
    "- The loss function typically used for regression problems is the mean square error: in this case you will have to figure out how to deal with the fact that the output vector has dimension two (transverse momentum, and rapidity).\n",
    "- A tricky challenge is to deal with output features that have different scales: the rapidity is of $\\mathcal{O}(1)$, the transverse momentum is of $\\\\mathcal{O}(100-1000}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772d79b-6dcd-4a2b-b9c1-5fb5b204771c",
   "metadata": {},
   "source": [
    "##Â Regression problems\n",
    "\n",
    "Regression problems require the prediction to be free of adopting the same range as the target variable(s) that need to be regressed.\n",
    "\n",
    "This is why the sigmoid activation function is not a good choice. The typical form of output layers of a regression problem is, if `n_outputs` is the dimension of the output vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbe240-aa59-40ce-9878-0877f6366a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(32, n_outputs),\n",
    "nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314e71e-c8f2-41ce-8107-370ed15f64fd",
   "metadata": {},
   "source": [
    "The other big change with respect to classification models is that the cross-entropy is not the proper loss function anymore.\n",
    "\n",
    "The regression problem is essentially a generalization of a linear regression problem, and the typical error estimates from classical statistics apply, each with its pros and cons.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$MAE(\\hat{y}, y^{*}) = \\frac{1}{N} \\sum |\\hat{y} - y^{*}|$\n",
    "\n",
    "- Lower values are better.\n",
    "- It estimates the average error, thus cannot distinguish between one large error and many small errors.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE(\\hat{y}, y^{*}) = \\sqrt{\\sum \\frac{(\\hat{y} - y^{*})^2}{N}}$\n",
    "\n",
    "- Lower values are better.\n",
    "- It estimates the spread of the residuals (standard deviation of the unexplained variance)\n",
    "- It gives large weight to large errors (if you use it as loss function, it will prioritize the reduction of large errors)\n",
    "\n",
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$MAPE(\\hat{y}, y^{*}) = \\frac{100\\%}{N} \\sum \\Big|\\frac{\\hat{y} - y^{*}}{y^{*}}\\Big|$\n",
    "\n",
    "#### R-Squared Score\n",
    "\n",
    "$R^2(\\hat{y}, y^{*}) = 1-\\frac{ \\sum (\\hat{y} - y^{*})^2}{  \\sum(\\bar{y} - y^{*})^2  }$, \n",
    "\n",
    "where $\\bar{y}$ is the arithmetic mean of the true values, $\\bar{y} = \\frac{1}{N}\\sum_{i=0}^{N-1} y^{*}$\n",
    "\n",
    "- It estimates how well the model explains the variance of the data\n",
    "- It can be negative (and that means that the model fits badly the data)\n",
    "\n",
    "\n",
    "You can consult online [an overview of the available loss functions in `pytorch`](https://pytorch.org/docs/stable/nn.html#loss-functions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd38be-7db1-4950-a55e-f1c17a0da3fd",
   "metadata": {},
   "source": [
    "## A few hints\n",
    "\n",
    "- Remove useless features and input features\n",
    "- Consider the possibility of applying preprocessing to the input features, to the target features, or to both\n",
    "- Choose the appropriate metric\n",
    "- Loss functions can be made as complicated as you want by defining your own loss function, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bdd1e-12d3-497f-bb5a-89402cd869c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class your_own_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ...\n",
    "\n",
    "loss_fn=your_own_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef484b5-fd1d-41e7-b060-1ff4f2c35f15",
   "metadata": {},
   "source": [
    "## The scoring system\n",
    "\n",
    "- You will have to define a model with two output nodes: the first one must regress the Higgs boson transverse momentum, the second one must regress the Higgs boson rapidity.\n",
    "- You can also use any flavour of boosted decision trees you may see fit, but implemented in `torch`.\n",
    "- Remember to select only `2lss` events (drop events with three leptons)\n",
    "- You will have to evaluate your model on the unlabelled data, save the predictions to a csv file with commas as separators (format: pt, y), and send the csv file [lisbon-ml-workshop@cern.ch](mailto:lisbon-ml-workshop@cern.ch). \n",
    "- If you have filtered the features further, please include in the email the code that creates the `data` dataframe.\n",
    "\n",
    "We will evaluate the results of the challenge on the unlabelled events, using as performance metric the RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134c6d0-cd20-4f8d-951f-39a4c1cad667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
