{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44264ed1-44ab-4ab3-a408-af3c643fc908",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/lisbon-ml-school/blob/master/dataChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8d289-3918-452e-b6e1-28ed97caa8e4",
   "metadata": {},
   "source": [
    "# Lisbon Machine Learning School\n",
    "## Data Challenge!!! Multitarget regression\n",
    "\n",
    "(C) Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49ac6-f688-46d1-82bb-c93ef1eb2cb6",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "- If you are running locally, you don't need to run anything\n",
    "\n",
    "- If you are running on Google Colab, uncomment and run the next cell (remove only the \"#\", keep the \"!\"). You can also run it from a local installation, but it will do nothing if you have already installed all dependencies (and it will take some time to tell you it is not gonna do anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9fe7a-5c11-4ce6-be45-402a10f052a0",
   "metadata": {},
   "source": [
    "## Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c894a5d9-5ab1-442b-af11-13bcf3f79f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchinfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "import uproot\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "print('Using torch version', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b52b21-e0e5-4c2a-9afb-e42afb99e42f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cf761-a0b2-496c-83bc-5344dca6902d",
   "metadata": {},
   "source": [
    "We will use the same data we used for exercise 2, that is simulated events corresponding to three physics processes.\n",
    "- ttH production\n",
    "- ttW production\n",
    "- Drell-Yan ($pp\\\\to Z/\\\\gamma^*$+jets) production\n",
    "\n",
    "We will select the multilepton final state, which is a challenging final state with a rich structure and nontrivial background separation.\n",
    "\n",
    "<img src=\"figs/2lss.png\" alt=\"ttH multilepton 2lss\" style=\"width:40%\"/>\n",
    "\n",
    "We use the [uproot](https://uproot.readthedocs.io/en/latest/basic.html) library to conveniently read in a [ROOT TNuple](https://root.cern.ch/doc/master/classTNtuple.html) which can automatically convert it to a [pandas dataframe](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74a60566-fd81-4c4b-b80c-1dafc8dc60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data only if you haven't done so yet\n",
    "\n",
    "if not os.path.isfile(\"data/signal_blind20.root\"): \n",
    "    !mkdir data; cd data/; wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz; tar xzvf lisbon_ml_school_tth.tar.gz; rm lisbon_ml_school_tth.tar.gz; cd -;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c829b510-e956-477d-a841-81b0a369ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = uproot.open('data/signal_blind20.root')['Friends'].arrays(library=\"pd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc7ae9-0d8e-4dac-b3bc-b8e986eb1654",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Select the features you want to use for this exercise, don't forget to remove unnecessary features.\n",
    "\n",
    "Most of the variables are input features, corresponding to detector measurements of the properties of the reconstructed decay products.\n",
    "\n",
    "There are three special variables, though:\n",
    "\n",
    "- `Hreco_evt_tag`: this feature has values in ${0,1}$, where $1$ flags the event as signal event, and $0$ flags the event as background event;\n",
    "- `Hreco_HTXS_Higgs_pt`: this feature contains the true generate Higgs boson transverse momentum at generator level (used for regression);\n",
    "- `Hreco_HTXS_Higgs_y`: this feature contains the true generated Higgs boson rapidity (not pseudorapidity) at generator level (used for regression).\n",
    "\n",
    "\n",
    "### Important\n",
    "\n",
    "Twenty percent of the events have `-99` in the `Hreco_HTXS_Higgs_pt` and `Hreco_HTXS_Higgs_y` values. These are the \"unlabelled\" events that you will have to send predictions for. You should filter them out for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dec0a6-c8da-4201-b7a9-675be0612864",
   "metadata": {},
   "source": [
    "## The assignment\n",
    "\n",
    "- For this data challenge, your target is to simultaneously regress the Higgs transverse momentum `Hreco_HTXS_Higgs_pt` and the rapidity `Hreco_HTXS_Higgs_y`\n",
    "\n",
    "- You will need to split your dataset into two parts: one is where you have access to the pT and y labels (80% of the dataset): you will build your training and test sets from this. The other is where the pT and y has been set to -99: this is the portion of data that is kept blind. You will have to use 80% of the data to train a regressor, then evaluate the output of your regressor on the blind 20% of the data, and send us the results. We will compare the result with the true value we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "406b6ce3-0f17-4d42-a581-1ad77a019490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Hreco_Lep0_pt', 'Hreco_Lep1_pt', 'Hreco_Lep2_pt',\n",
      "       'Hreco_HadTop_pt', 'Hreco_All5_Jets_pt', 'Hreco_More5_Jets_pt',\n",
      "       'Hreco_Jets_plus_Lep_pt', 'Hreco_Lep0_eta', 'Hreco_Lep1_eta',\n",
      "       'Hreco_Lep2_eta', 'Hreco_HadTop_eta', 'Hreco_All5_Jets_eta',\n",
      "       'Hreco_More5_Jets_eta', 'Hreco_Jets_plus_Lep_eta', 'Hreco_Lep0_phi',\n",
      "       'Hreco_Lep1_phi', 'Hreco_Lep2_phi', 'Hreco_HadTop_phi',\n",
      "       'Hreco_All5_Jets_phi', 'Hreco_More5_Jets_phi',\n",
      "       'Hreco_Jets_plus_Lep_phi', 'Hreco_Lep0_mass', 'Hreco_Lep1_mass',\n",
      "       'Hreco_Lep2_mass', 'Hreco_HadTop_mass', 'Hreco_All5_Jets_mass',\n",
      "       'Hreco_More5_Jets_mass', 'Hreco_Jets_plus_Lep_mass', 'Hreco_TopScore',\n",
      "       'Hreco_met', 'Hreco_met_phi', 'Hreco_HTXS_Higgs_pt',\n",
      "       'Hreco_HTXS_Higgs_y', 'Hreco_evt_tag'],\n",
      "      dtype='object')\n",
      "0.19994854437379506 59842\n",
      "0.8000514556262049 239445\n"
     ]
    }
   ],
   "source": [
    "# Filter data\n",
    "print(data.columns)\n",
    "# Drop unneeded features\n",
    "\n",
    "#data = data.drop([\"index\"], axis=1 )\n",
    "blind_data = data[data[\"Hreco_HTXS_Higgs_pt\"]==-99]\n",
    "\n",
    "train_test_data = data[data[\"Hreco_HTXS_Higgs_pt\"]!=-99]\n",
    "\n",
    "print(blind_data.shape[0] / data.shape[0], blind_data.shape[0])\n",
    "print(train_test_data.shape[0] / data.shape[0], train_test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a72e29-98fb-41ac-b178-30402b03968a",
   "metadata": {},
   "source": [
    "- The loss function typically used for regression problems is the mean square error: in this case you will have to figure out how to deal with the fact that the output vector has dimension two (transverse momentum, and rapidity).\n",
    "- A tricky challenge is to deal with output features that have different scales: the rapidity is of $\\mathcal{O}(1)$, the transverse momentum is of $\\\\mathcal{O}(100-1000}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772d79b-6dcd-4a2b-b9c1-5fb5b204771c",
   "metadata": {},
   "source": [
    "##Â Regression problems\n",
    "\n",
    "Regression problems require the prediction to be free of adopting the same range as the target variable(s) that need to be regressed.\n",
    "\n",
    "This is why the sigmoid activation function is not a good choice. The typical form of output layers of a regression problem is, if `n_outputs` is the dimension of the output vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbe240-aa59-40ce-9878-0877f6366a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(32, n_outputs),\n",
    "nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314e71e-c8f2-41ce-8107-370ed15f64fd",
   "metadata": {},
   "source": [
    "The other big change with respect to classification models is that the cross-entropy is not the proper loss function anymore.\n",
    "\n",
    "The regression problem is essentially a generalization of a linear regression problem, and the typical error estimates from classical statistics apply, each with its pros and cons.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$MAE(\\hat{y}, y^{*}) = \\frac{1}{N} \\sum |\\hat{y} - y^{*}|$\n",
    "\n",
    "- Lower values are better.\n",
    "- It estimates the average error, thus cannot distinguish between one large error and many small errors.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE(\\hat{y}, y^{*}) = \\sqrt{\\sum \\frac{(\\hat{y} - y^{*})^2}{N}}$\n",
    "\n",
    "- Lower values are better.\n",
    "- It estimates the spread of the residuals (standard deviation of the unexplained variance)\n",
    "- It gives large weight to large errors (if you use it as loss function, it will prioritize the reduction of large errors)\n",
    "\n",
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$MAPE(\\hat{y}, y^{*}) = \\frac{100\\%}{N} \\sum \\Big|\\frac{\\hat{y} - y^{*}}{y^{*}}\\Big|$\n",
    "\n",
    "#### R-Squared Score\n",
    "\n",
    "$R^2(\\hat{y}, y^{*}) = 1-\\frac{ \\sum (\\hat{y} - y^{*})^2}{  \\sum(\\bar{y} - y^{*})^2  }$, \n",
    "\n",
    "where $\\bar{y}$ is the arithmetic mean of the true values, $\\bar{y} = \\frac{1}{N}\\sum_{i=0}^{N-1} y^{*}$\n",
    "\n",
    "- It estimates how well the model explains the variance of the data\n",
    "- It can be negative (and that means that the model fits badly the data)\n",
    "\n",
    "\n",
    "You can consult online [an overview of the available loss functions in `pytorch`](https://pytorch.org/docs/stable/nn.html#loss-functions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd38be-7db1-4950-a55e-f1c17a0da3fd",
   "metadata": {},
   "source": [
    "## A few hints\n",
    "\n",
    "- Remove useless features\n",
    "- Consider the possibility of applying preprocessing to the input features, to the target features, or to both\n",
    "- Choose the appropriate metric to track\n",
    "- You can recycle the code for DataSet, DataLoader, Neural Network model, and train/test loops from exercise_1 essentially verbatim. Just make sure you change the loss function, and you change the output activation function to `nnReLU()`\n",
    "- Loss functions can be made as complicated as you want by defining your own loss function, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bdd1e-12d3-497f-bb5a-89402cd869c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class your_own_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return ...\n",
    "\n",
    "loss_fn=your_own_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef484b5-fd1d-41e7-b060-1ff4f2c35f15",
   "metadata": {},
   "source": [
    "## The scoring system\n",
    "\n",
    "- You will have to define a model with two output nodes: the first one must regress the Higgs boson transverse momentum, the second one must regress the Higgs boson rapidity.\n",
    "- You can also use any flavour of boosted decision trees you may see fit, but implemented in `torch`.\n",
    "- You will have to evaluate your model on the unlabelled data, save the predictions to a csv file with commas as separators (format: pt, y), and send us the csv file (see below). \n",
    "- If you have filtered the features further, please include in the email the code that creates the `data` dataframe.\n",
    "\n",
    "We will evaluate the results of the challenge on the unlabelled events, using as performance metric a weighted RMSE (the two features weight by their standardization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc4170-4898-477f-acab-125aecbb9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_send = model(blind_data_X_features).cpu().numpy()\n",
    "\n",
    "df_to_send = pd.DataFrame(to_send)\n",
    "df.to_csv(\"my_file_group_N\",index=False) # Replace \"N\" with the group number we assigned you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22ec80-956b-45ab-9e0d-58325019a516",
   "metadata": {},
   "source": [
    "# Practical instructions\n",
    "\n",
    "- Divide yourselves in groups of three. Once the groups are defined, we will assign you a numerical label.\n",
    "- Each group will have to send the csv file mentioned above as an attached `.csv`file to [lisbon-ml-workshop@cern.ch](mailto:lisbon-ml-workshop@cern.ch). The email (ONE EMAIL PER GROUP) must have:\n",
    "  - As an object: \"LIP ML Workshop Data Challenge: Group N\", where \"N\" is the number we assigned you above\n",
    "  - All the three members of the group must be in carbon copy to the email\n",
    "  - In the email text there must be a list of the full names of the three members of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e742f57-0770-4043-b0e0-b50441146557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
